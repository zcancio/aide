"""
AIde Engine - Single-file Python kernel

Complete event-sourced kernel for AIde living objects.
Pure functions: events → reducer → snapshot

This file is generated by scripts/build_engine_python.py
DO NOT EDIT MANUALLY - Edit source files in engine/kernel/ instead

Usage:
  from engine import AideAssembly, Blueprint, Event
  from engine import reduce, empty_state, base_type, resolve_view_entities
  # React renders from snapshot directly

Reference: docs/eng_design/aide_architecture.md
"""

from __future__ import annotations

# Standard library imports
import asyncio
import copy
import json
import re
import uuid
from dataclasses import dataclass, field
from datetime import datetime, timezone
from html import escape as _html_escape
from typing import Any


# ===========================================================================
# KERNEL MODULES
# ===========================================================================


# ---------------------------------------------------------------------------
# types.py
# ---------------------------------------------------------------------------

"""

AIde Kernel — Shared Types

Data classes used across primitives, reducer, renderer, and assembly.
These are the contracts that bind the kernel together.

"""


import re
from dataclasses import dataclass, field
from datetime import UTC, datetime
from typing import Any

# ---------------------------------------------------------------------------
# Regex patterns (from aide_primitive_schemas.md)
# ---------------------------------------------------------------------------

ID_PATTERN = re.compile(r"^[a-z][a-z0-9_]{0,63}$")
REF_PATTERN = re.compile(r"^[a-z][a-z0-9_]{0,63}/[a-z][a-z0-9_]{0,63}$")


# ---------------------------------------------------------------------------
# Primitive type registry
# ---------------------------------------------------------------------------

PRIMITIVE_TYPES: set[str] = {
    # Entity (1-3)
    "entity.create",
    "entity.update",
    "entity.remove",
    # Collection (4-6)
    "collection.create",
    "collection.update",
    "collection.remove",
    # Grid (batch entity creation, query)
    "grid.create",
    "grid.query",
    # Field (7-9)
    "field.add",
    "field.update",
    "field.remove",
    # Relationship (10-11)
    "relationship.set",
    "relationship.constrain",
    # Block (12-14)
    "block.set",
    "block.remove",
    "block.reorder",
    # View (15-17)
    "view.create",
    "view.update",
    "view.remove",
    # Style (18-19)
    "style.set",
    "style.set_entity",
    # Meta (20-22)
    "meta.update",
    "meta.annotate",
    "meta.constrain",
}

BLOCK_TYPES: set[str] = {
    "root",
    "heading",
    "text",
    "metric",
    "collection_view",
    "divider",
    "image",
    "callout",
    "column_list",
    "column",
}

VIEW_TYPES: set[str] = {
    "list",
    "table",
    "grid",
}

# Field types — simple string types and their nullable variants
SIMPLE_FIELD_TYPES: set[str] = {
    "string",
    "string?",
    "int",
    "int?",
    "float",
    "float?",
    "bool",
    "date",
    "date?",
    "datetime",
    "datetime?",
}

# Known style tokens and their defaults
DEFAULT_STYLES: dict[str, str] = {
    "primary_color": "#2D2D2A",
    "bg_color": "#F7F5F2",
    "text_color": "#2D2D2A",
    "font_family": "DM Sans",
    "heading_font": "Playfair Display",
    "density": "comfortable",
}

DENSITY_VALUES: set[str] = {"compact", "comfortable", "spacious"}

# Known meta properties
KNOWN_META_PROPERTIES: set[str] = {"title", "identity", "visibility", "archived"}

# Constraint rule types
CONSTRAINT_RULES: set[str] = {
    "exclude_pair",
    "require_same",
    "max_per_target",
    "min_per_target",
    "collection_max_entities",
    "collection_min_entities",
    "unique_field",
    "required_fields",
}

# Escalation reasons
ESCALATION_REASONS: set[str] = {
    "no_schema",
    "unknown_entity",
    "unknown_field",
    "novel_view",
    "structural_change",
    "ambiguous",
    "complex_conditional",
}


# ---------------------------------------------------------------------------
# Data classes
# ---------------------------------------------------------------------------


@dataclass
class Snapshot:
    """
    The aide's current state — matches the reducer's expected structure.

    Note: Entities are stored INSIDE each collection as collection["entities"],
    not at the top level. This matches how the reducer processes state.
    """

    version: int = 1
    meta: dict[str, Any] = field(default_factory=dict)
    collections: dict[str, Any] = field(default_factory=dict)  # Each collection has "entities" inside
    relationships: list[dict[str, Any]] = field(default_factory=list)
    relationship_types: dict[str, Any] = field(default_factory=dict)
    constraints: list[dict[str, Any]] = field(default_factory=list)
    blocks: dict[str, Any] = field(default_factory=lambda: {"block_root": {"type": "root", "children": []}})
    views: dict[str, Any] = field(default_factory=dict)
    styles: dict[str, Any] = field(default_factory=dict)
    annotations: list[dict[str, Any]] = field(default_factory=list)

    def to_dict(self) -> dict[str, Any]:
        return {
            "version": self.version,
            "meta": self.meta,
            "collections": self.collections,
            "relationships": self.relationships,
            "relationship_types": self.relationship_types,
            "constraints": self.constraints,
            "blocks": self.blocks,
            "views": self.views,
            "styles": self.styles,
            "annotations": self.annotations,
        }

    @classmethod
    def from_dict(cls, d: dict[str, Any]) -> Snapshot:
        return cls(
            version=d.get("version", 1),
            meta=d.get("meta", {}),
            collections=d.get("collections", {}),
            relationships=d.get("relationships", []),
            relationship_types=d.get("relationship_types", {}),
            constraints=d.get("constraints", []),
            blocks=d.get("blocks", {"block_root": {"type": "root", "children": []}}),
            views=d.get("views", {}),
            styles=d.get("styles", {}),
            annotations=d.get("annotations", []),
        )


@dataclass
class Event:
    """
    Wraps a primitive with metadata for the append-only event log.
    The reducer reads only `type` and `payload`.
    """

    id: str
    sequence: int
    timestamp: str  # ISO 8601 UTC
    actor: str
    source: str
    type: str
    payload: dict[str, Any]
    intent: str | None = None
    message: str | None = None
    message_id: str | None = None

    def to_dict(self) -> dict[str, Any]:
        d: dict[str, Any] = {
            "id": self.id,
            "sequence": self.sequence,
            "timestamp": self.timestamp,
            "actor": self.actor,
            "source": self.source,
            "type": self.type,
            "payload": self.payload,
        }
        if self.intent is not None:
            d["intent"] = self.intent
        if self.message is not None:
            d["message"] = self.message
        if self.message_id is not None:
            d["message_id"] = self.message_id
        return d

    @classmethod
    def from_dict(cls, d: dict[str, Any]) -> Event:
        return cls(
            id=d["id"],
            sequence=d["sequence"],
            timestamp=d["timestamp"],
            actor=d["actor"],
            source=d["source"],
            type=d["type"],
            payload=d["payload"],
            intent=d.get("intent"),
            message=d.get("message"),
            message_id=d.get("message_id"),
        )


@dataclass
class Warning:
    """A non-fatal issue encountered during reduction."""

    code: str
    message: str
    details: dict[str, Any] | None = None


@dataclass
class ReduceResult:
    """
    Result of applying one event to a snapshot.
    The reducer never throws — it always returns one of these.
    """

    snapshot: dict[str, Any]  # AideState
    applied: bool
    warnings: list[Warning] = field(default_factory=list)
    error: str | None = None


@dataclass
class Blueprint:
    """
    The aide's DNA — identity, voice rules, and LLM system prompt.
    Embedded in the HTML file for portability.
    """

    identity: str
    voice: str = "No first person. State reflections only."
    prompt: str = ""

    def to_dict(self) -> dict[str, Any]:
        return {
            "identity": self.identity,
            "voice": self.voice,
            "prompt": self.prompt,
        }

    @classmethod
    def from_dict(cls, d: dict[str, Any]) -> Blueprint:
        return cls(
            identity=d.get("identity", ""),
            voice=d.get("voice", "No first person. State reflections only."),
            prompt=d.get("prompt", ""),
        )


@dataclass
class RenderOptions:
    """Options controlling what the renderer includes in output."""

    include_events: bool = True
    include_blueprint: bool = True
    include_fonts: bool = True
    footer: str | None = None  # "Made with AIde" for free tier
    base_url: str = "https://toaide.com"


@dataclass
class AideFile:
    """In-memory representation of a loaded aide HTML file."""

    aide_id: str
    snapshot: dict[str, Any]  # AideState
    events: list[Event]
    blueprint: Blueprint
    html: str
    last_sequence: int
    size_bytes: int
    loaded_from: str  # "r2" or "new"


@dataclass
class ApplyResult:
    """Result of applying a batch of events through the assembly layer."""

    aide_file: AideFile
    applied: list[Event]
    rejected: list[tuple[Event, str]]  # (event, error_reason)
    warnings: list[Warning]


@dataclass
class ParsedAide:
    """Result of parsing an existing aide HTML file."""

    blueprint: Blueprint | None
    snapshot: dict[str, Any] | None
    events: list[Event]
    parse_errors: list[str]


@dataclass
class Escalation:
    """Signal from L2 when it can't compile a user message into primitives."""

    reason: str
    user_message: str
    context: str
    attempted: dict[str, Any] | None = None

    def to_dict(self) -> dict[str, Any]:
        return {
            "type": "escalation",
            "reason": self.reason,
            "user_message": self.user_message,
            "context": self.context,
            "attempted": self.attempted,
        }


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------


def parse_ref(ref: str) -> tuple[str, str]:
    """Parse 'collection_id/entity_id' into (collection_id, entity_id)."""
    parts = ref.split("/", 1)
    if len(parts) != 2:
        raise ValueError(f"Invalid ref format: {ref}")
    return parts[0], parts[1]


def is_valid_id(value: str) -> bool:
    """Check if a string is a valid AIde ID (snake_case, max 64 chars)."""
    return bool(ID_PATTERN.match(value))


def is_valid_ref(value: str) -> bool:
    """Check if a string is a valid entity ref (collection_id/entity_id)."""
    return bool(REF_PATTERN.match(value))


def is_nullable_type(field_type: str | dict) -> bool:
    """Check if a field type is nullable (ends with ?)."""
    if isinstance(field_type, str):
        return field_type.endswith("?")
    # Complex types (enum, list) are not nullable
    return False


def base_type(field_type: str | dict) -> str:
    """Get the base type name without nullable suffix."""
    if isinstance(field_type, str):
        return field_type.rstrip("?")
    if isinstance(field_type, dict):
        if "enum" in field_type:
            return "enum"
        if "list" in field_type:
            return "list"
    return "unknown"


def is_valid_field_type(field_type: str | dict) -> bool:
    """Check if a field type definition is valid."""
    if isinstance(field_type, str):
        return field_type in SIMPLE_FIELD_TYPES
    if isinstance(field_type, dict):
        if "enum" in field_type:
            vals = field_type["enum"]
            return isinstance(vals, list) and len(vals) > 0 and all(isinstance(v, str) for v in vals)
        if "list" in field_type:
            inner = field_type["list"]
            return isinstance(inner, str) and inner in {"string", "int", "float"}
    return False


def now_iso() -> str:
    """Current UTC time as ISO 8601 string."""
    return datetime.now(UTC).strftime("%Y-%m-%dT%H:%M:%SZ")



# ---------------------------------------------------------------------------
# primitives.py
# ---------------------------------------------------------------------------

"""

AIde Kernel — Primitive Validation

Validates primitive payloads before they reach the reducer.
Every state change goes through one of 22 primitive types.
Validation is structural (well-formed?) not semantic (will it apply?).
The reducer handles semantic checks (does the collection exist? etc.).

Reference: aide_primitive_schemas.md

"""


from typing import Any


# ---------------------------------------------------------------------------
# Public API
# ---------------------------------------------------------------------------


def validate_primitive(type: str, payload: dict[str, Any]) -> list[str]:
    """
    Validate a primitive's type and payload structure.
    Returns a list of error strings. Empty list = valid.

    This checks structural validity only:
    - Is the type recognized?
    - Is the payload a dict?
    - Are required fields present?
    - Are IDs well-formed?
    - Are refs well-formed?

    It does NOT check whether referenced entities/collections exist.
    That's the reducer's job.
    """
    errors: list[str] = []

    # Universal: type must be known
    if type not in PRIMITIVE_TYPES:
        errors.append(f"Unknown primitive type: {type}")
        return errors  # can't validate payload for unknown type

    # Universal: payload must be a dict
    if not isinstance(payload, dict):
        errors.append("Payload must be a non-null object")
        return errors

    # Dispatch to type-specific validator
    validator = _VALIDATORS.get(type)
    if validator:
        errors.extend(validator(payload))

    return errors


# ---------------------------------------------------------------------------
# Per-primitive validators
# ---------------------------------------------------------------------------


def _validate_entity_create(p: dict) -> list[str]:
    errors: list[str] = []
    if "collection" not in p:
        errors.append("entity.create requires 'collection'")
    elif not is_valid_id(p["collection"]):
        errors.append(f"Invalid collection ID: {p['collection']}")

    if "id" in p and p["id"] is not None and not is_valid_id(p["id"]):
        errors.append(f"Invalid entity ID: {p['id']}")

    if "fields" not in p:
        errors.append("entity.create requires 'fields'")
    elif not isinstance(p["fields"], dict):
        errors.append("'fields' must be an object")

    return errors


def _validate_entity_update(p: dict) -> list[str]:
    errors: list[str] = []
    has_ref = "ref" in p
    has_filter = "filter" in p
    has_cell_ref = "cell_ref" in p  # Grid cell reference (resolved by backend)

    if not has_ref and not has_filter and not has_cell_ref:
        errors.append("entity.update requires 'ref', 'filter', or 'cell_ref'")
    elif sum([has_ref, has_filter, has_cell_ref]) > 1:
        errors.append("entity.update: provide only one of 'ref', 'filter', or 'cell_ref'")

    if has_ref and not is_valid_ref(p["ref"]):
        errors.append(f"Invalid ref: {p['ref']}")

    if has_filter:
        f = p["filter"]
        if not isinstance(f, dict):
            errors.append("'filter' must be an object")
        elif "collection" not in f:
            errors.append("filter requires 'collection'")

    if has_cell_ref:
        if not isinstance(p["cell_ref"], str):
            errors.append("'cell_ref' must be a string")
        if "collection" not in p:
            errors.append("cell_ref requires 'collection'")

    if "fields" not in p:
        errors.append("entity.update requires 'fields'")
    elif not isinstance(p["fields"], dict):
        errors.append("'fields' must be an object")

    return errors


def _validate_entity_remove(p: dict) -> list[str]:
    errors: list[str] = []
    if "ref" not in p:
        errors.append("entity.remove requires 'ref'")
    elif not is_valid_ref(p["ref"]):
        errors.append(f"Invalid ref: {p['ref']}")
    return errors


def _validate_collection_create(p: dict) -> list[str]:
    errors: list[str] = []
    if "id" not in p:
        errors.append("collection.create requires 'id'")
    elif not is_valid_id(p["id"]):
        errors.append(f"Invalid collection ID: {p['id']}")

    if "schema" not in p:
        errors.append("collection.create requires 'schema'")
    elif not isinstance(p["schema"], dict):
        errors.append("'schema' must be an object")
    else:
        for field_name, field_type in p["schema"].items():
            if not is_valid_id(field_name):
                errors.append(f"Invalid field name in schema: {field_name}")
            if not is_valid_field_type(field_type):
                errors.append(f"Invalid field type for '{field_name}': {field_type}")

    return errors


def _validate_collection_update(p: dict) -> list[str]:
    errors: list[str] = []
    if "id" not in p:
        errors.append("collection.update requires 'id'")
    elif not is_valid_id(p["id"]):
        errors.append(f"Invalid collection ID: {p['id']}")
    return errors


def _validate_collection_remove(p: dict) -> list[str]:
    errors: list[str] = []
    if "id" not in p:
        errors.append("collection.remove requires 'id'")
    elif not is_valid_id(p["id"]):
        errors.append(f"Invalid collection ID: {p['id']}")
    return errors


def _validate_grid_create(p: dict) -> list[str]:
    """Validate grid.create primitive for batch entity creation."""
    errors: list[str] = []
    if "collection" not in p:
        errors.append("grid.create requires 'collection'")
    elif not is_valid_id(p["collection"]):
        errors.append(f"Invalid collection ID: {p['collection']}")

    if "rows" not in p:
        errors.append("grid.create requires 'rows'")
    elif not isinstance(p["rows"], int) or p["rows"] < 1:
        errors.append("'rows' must be a positive integer")

    if "cols" not in p:
        errors.append("grid.create requires 'cols'")
    elif not isinstance(p["cols"], int) or p["cols"] < 1:
        errors.append("'cols' must be a positive integer")

    return errors


def _validate_grid_query(p: dict) -> list[str]:
    """Validate grid.query primitive for cell lookups."""
    errors: list[str] = []
    if "cell_ref" not in p:
        errors.append("grid.query requires 'cell_ref'")
    elif not isinstance(p["cell_ref"], str):
        errors.append("'cell_ref' must be a string")

    if "collection" not in p:
        errors.append("grid.query requires 'collection'")
    elif not is_valid_id(p["collection"]):
        errors.append(f"Invalid collection ID: {p['collection']}")

    return errors


def _validate_field_add(p: dict) -> list[str]:
    errors: list[str] = []
    if "collection" not in p:
        errors.append("field.add requires 'collection'")
    elif not is_valid_id(p["collection"]):
        errors.append(f"Invalid collection ID: {p['collection']}")

    if "name" not in p:
        errors.append("field.add requires 'name'")
    elif not is_valid_id(p["name"]):
        errors.append(f"Invalid field name: {p['name']}")

    if "type" not in p:
        errors.append("field.add requires 'type'")
    elif not is_valid_field_type(p["type"]):
        errors.append(f"Invalid field type: {p['type']}")

    return errors


def _validate_field_update(p: dict) -> list[str]:
    errors: list[str] = []
    if "collection" not in p:
        errors.append("field.update requires 'collection'")
    elif not is_valid_id(p["collection"]):
        errors.append(f"Invalid collection ID: {p['collection']}")

    if "name" not in p:
        errors.append("field.update requires 'name'")
    elif not is_valid_id(p["name"]):
        errors.append(f"Invalid field name: {p['name']}")

    return errors


def _validate_field_remove(p: dict) -> list[str]:
    errors: list[str] = []
    if "collection" not in p:
        errors.append("field.remove requires 'collection'")
    elif not is_valid_id(p["collection"]):
        errors.append(f"Invalid collection ID: {p['collection']}")

    if "name" not in p:
        errors.append("field.remove requires 'name'")
    elif not is_valid_id(p["name"]):
        errors.append(f"Invalid field name: {p['name']}")

    return errors


def _validate_relationship_set(p: dict) -> list[str]:
    errors: list[str] = []
    for key in ("from", "to", "type"):
        if key not in p:
            errors.append(f"relationship.set requires '{key}'")

    if "from" in p and not is_valid_ref(p["from"]):
        errors.append(f"Invalid 'from' ref: {p['from']}")
    if "to" in p and not is_valid_ref(p["to"]):
        errors.append(f"Invalid 'to' ref: {p['to']}")
    if "type" in p and not is_valid_id(p["type"]):
        errors.append(f"Invalid relationship type: {p['type']}")

    return errors


def _validate_relationship_constrain(p: dict) -> list[str]:
    errors: list[str] = []
    if "id" not in p:
        errors.append("relationship.constrain requires 'id'")
    elif not is_valid_id(p["id"]):
        errors.append(f"Invalid constraint ID: {p['id']}")

    if "rule" not in p:
        errors.append("relationship.constrain requires 'rule'")
    elif p["rule"] not in CONSTRAINT_RULES:
        errors.append(f"Unknown constraint rule: {p['rule']}")

    return errors


def _validate_block_set(p: dict) -> list[str]:
    errors: list[str] = []
    if "id" not in p:
        errors.append("block.set requires 'id'")
    elif not is_valid_id(p["id"]):
        errors.append(f"Invalid block ID: {p['id']}")

    # type is required for creation, optional for update — reducer decides
    if "type" in p and p["type"] not in BLOCK_TYPES:
        errors.append(f"Unknown block type: {p['type']}")

    if "parent" in p and not is_valid_id(p["parent"]):
        errors.append(f"Invalid parent block ID: {p['parent']}")

    return errors


def _validate_block_remove(p: dict) -> list[str]:
    errors: list[str] = []
    if "id" not in p:
        errors.append("block.remove requires 'id'")
    elif not is_valid_id(p["id"]):
        errors.append(f"Invalid block ID: {p['id']}")
    return errors


def _validate_block_reorder(p: dict) -> list[str]:
    errors: list[str] = []
    if "parent" not in p:
        errors.append("block.reorder requires 'parent'")
    elif not is_valid_id(p["parent"]):
        errors.append(f"Invalid parent block ID: {p['parent']}")

    if "children" not in p:
        errors.append("block.reorder requires 'children'")
    elif not isinstance(p["children"], list):
        errors.append("'children' must be a list")
    else:
        for child_id in p["children"]:
            if not isinstance(child_id, str) or not is_valid_id(child_id):
                errors.append(f"Invalid child block ID: {child_id}")
    return errors


def _validate_view_create(p: dict) -> list[str]:
    errors: list[str] = []
    if "id" not in p:
        errors.append("view.create requires 'id'")
    elif not is_valid_id(p["id"]):
        errors.append(f"Invalid view ID: {p['id']}")

    if "type" not in p:
        errors.append("view.create requires 'type'")
    elif p["type"] not in VIEW_TYPES:
        errors.append(f"Unknown view type: {p['type']}. Known: {VIEW_TYPES}")

    if "source" not in p:
        errors.append("view.create requires 'source'")
    elif not is_valid_id(p["source"]):
        errors.append(f"Invalid source collection ID: {p['source']}")

    return errors


def _validate_view_update(p: dict) -> list[str]:
    errors: list[str] = []
    if "id" not in p:
        errors.append("view.update requires 'id'")
    elif not is_valid_id(p["id"]):
        errors.append(f"Invalid view ID: {p['id']}")

    if "type" in p and p["type"] not in VIEW_TYPES:
        errors.append(f"Unknown view type: {p['type']}")

    return errors


def _validate_view_remove(p: dict) -> list[str]:
    errors: list[str] = []
    if "id" not in p:
        errors.append("view.remove requires 'id'")
    elif not is_valid_id(p["id"]):
        errors.append(f"Invalid view ID: {p['id']}")
    return errors


def _validate_style_set(p: dict) -> list[str]:
    # All keys accepted — unknown tokens stored for forward compatibility
    return []


def _validate_style_set_entity(p: dict) -> list[str]:
    errors: list[str] = []
    if "ref" not in p:
        errors.append("style.set_entity requires 'ref'")
    elif not is_valid_ref(p["ref"]):
        errors.append(f"Invalid ref: {p['ref']}")

    if "styles" not in p:
        errors.append("style.set_entity requires 'styles'")
    elif not isinstance(p["styles"], dict):
        errors.append("'styles' must be an object")

    return errors


def _validate_meta_update(p: dict) -> list[str]:
    # All keys accepted — unknown properties stored
    return []


def _validate_meta_annotate(p: dict) -> list[str]:
    errors: list[str] = []
    if "note" not in p:
        errors.append("meta.annotate requires 'note'")
    elif not isinstance(p["note"], str):
        errors.append("'note' must be a string")
    return errors


def _validate_meta_constrain(p: dict) -> list[str]:
    errors: list[str] = []
    if "id" not in p:
        errors.append("meta.constrain requires 'id'")
    elif not is_valid_id(p["id"]):
        errors.append(f"Invalid constraint ID: {p['id']}")

    if "rule" not in p:
        errors.append("meta.constrain requires 'rule'")
    elif p["rule"] not in CONSTRAINT_RULES:
        errors.append(f"Unknown constraint rule: {p['rule']}")

    return errors


# ---------------------------------------------------------------------------
# Dispatcher
# ---------------------------------------------------------------------------

_VALIDATORS: dict[str, Any] = {
    "entity.create": _validate_entity_create,
    "entity.update": _validate_entity_update,
    "entity.remove": _validate_entity_remove,
    "collection.create": _validate_collection_create,
    "collection.update": _validate_collection_update,
    "collection.remove": _validate_collection_remove,
    "grid.create": _validate_grid_create,
    "grid.query": _validate_grid_query,
    "field.add": _validate_field_add,
    "field.update": _validate_field_update,
    "field.remove": _validate_field_remove,
    "relationship.set": _validate_relationship_set,
    "relationship.constrain": _validate_relationship_constrain,
    "block.set": _validate_block_set,
    "block.remove": _validate_block_remove,
    "block.reorder": _validate_block_reorder,
    "view.create": _validate_view_create,
    "view.update": _validate_view_update,
    "view.remove": _validate_view_remove,
    "style.set": _validate_style_set,
    "style.set_entity": _validate_style_set_entity,
    "meta.update": _validate_meta_update,
    "meta.annotate": _validate_meta_annotate,
    "meta.constrain": _validate_meta_constrain,
}



# ---------------------------------------------------------------------------
# reducer.py
# ---------------------------------------------------------------------------

"""

AIde Kernel — Reducer

Pure function: (snapshot, event) → ReduceResult
No side effects. No IO. No AI calls. Deterministic.

Given the same sequence of events, produces the same snapshot every time.

Reference: aide_reducer_spec.md

"""


import copy
from typing import Any


# ---------------------------------------------------------------------------
# Public API
# ---------------------------------------------------------------------------


def empty_state() -> dict[str, Any]:
    """
    The initial snapshot when an aide has zero events.
    Only block_root exists.
    """
    return {
        "version": 1,
        "meta": {},
        "collections": {},
        "relationships": [],
        "relationship_types": {},
        "constraints": [],
        "blocks": {
            "block_root": {"type": "root", "children": []},
        },
        "views": {},
        "styles": {},
        "annotations": [],
    }


def reduce(snapshot: dict[str, Any], event: Event) -> ReduceResult:
    """
    Apply one event to the current snapshot.
    Returns new snapshot + applied flag + warnings/errors.

    Pure function. The returned snapshot is a new dict (deep copy on mutation paths).
    The input snapshot is never modified.
    """
    handler = _HANDLERS.get(event.type)
    if handler is None:
        return ReduceResult(
            snapshot=snapshot,
            applied=False,
            error=f"UNKNOWN_PRIMITIVE: {event.type}",
        )

    # Deep copy so we never mutate the input
    snap = copy.deepcopy(snapshot)
    return handler(snap, event)


def replay(events: list[Event]) -> dict[str, Any]:
    """
    Rebuild snapshot from scratch by reducing over all events.
    replay(events) == reduce(reduce(reduce(empty(), e1), e2), e3)...
    """
    snapshot = empty_state()
    for event in events:
        result = reduce(snapshot, event)
        if result.applied:
            snapshot = result.snapshot
    return snapshot


# ---------------------------------------------------------------------------
# Internal helpers
# ---------------------------------------------------------------------------


def _reject(snap: dict, code: str, msg: str) -> ReduceResult:
    return ReduceResult(snapshot=snap, applied=False, error=f"{code}: {msg}")


def _ok(snap: dict, warnings: list[Warning] | None = None) -> ReduceResult:
    return ReduceResult(snapshot=snap, applied=True, warnings=warnings or [])


def _get_collection(snap: dict, coll_id: str) -> dict | None:
    """Lookup a non-removed collection."""
    coll = snap["collections"].get(coll_id)
    if coll is None or coll.get("_removed"):
        return None
    return coll


def _get_entity(snap: dict, coll_id: str, entity_id: str) -> tuple[dict | None, dict | None]:
    """Lookup a non-removed entity. Returns (collection, entity) or (None, None)."""
    coll = _get_collection(snap, coll_id)
    if coll is None:
        return None, None
    entity = coll["entities"].get(entity_id)
    if entity is None or entity.get("_removed"):
        return coll, None
    return coll, entity


def _validate_field_value(value: Any, field_type: str | dict) -> tuple[bool, str | None]:
    """
    Check if a value conforms to a schema field type.
    Returns (valid, error_message).
    """
    if value is None:
        if is_nullable_type(field_type):
            return True, None
        return False, "null not allowed for non-nullable type"

    bt = base_type(field_type)

    if bt == "string":
        return isinstance(value, str), "expected string"
    if bt == "int":
        return isinstance(value, int) and not isinstance(value, bool), "expected int"
    if bt == "float":
        return isinstance(value, int | float) and not isinstance(value, bool), "expected float"
    if bt == "bool":
        return isinstance(value, bool), "expected bool"
    if bt == "date":
        return isinstance(value, str), "expected date string"  # further validation could check ISO format
    if bt == "datetime":
        return isinstance(value, str), "expected datetime string"
    if bt == "enum":
        if isinstance(field_type, dict) and "enum" in field_type:
            return value in field_type["enum"], f"expected one of {field_type['enum']}"
        return False, "malformed enum type"
    if bt == "list":
        if not isinstance(value, list):
            return False, "expected list"
        if isinstance(field_type, dict) and "list" in field_type:
            inner = field_type["list"]
            for item in value:
                ok, _ = _validate_field_value(item, inner)
                if not ok:
                    return False, f"list item type mismatch: expected {inner}"
        return True, None

    return True, None  # Unknown types pass (forward compat)


def _check_constraints(snap: dict, event: Event, warnings: list[Warning]) -> bool:
    """
    Check relevant constraints after an event is applied.
    Returns False if a strict constraint was violated (event should be rejected).
    Adds warnings for non-strict violations.
    """
    for constraint in snap["constraints"]:
        rule = constraint.get("rule")
        strict = constraint.get("strict", False)
        violated = False
        msg = constraint.get("message", f"Constraint {constraint.get('id')} violated")

        if rule == "collection_max_entities" and event.type in ("entity.create",):
            coll_id = constraint.get("collection")
            max_val = constraint.get("value")
            if coll_id and max_val is not None:
                coll = _get_collection(snap, coll_id)
                if coll:
                    count = sum(1 for e in coll["entities"].values() if not e.get("_removed"))
                    if count > max_val:
                        violated = True

        elif rule == "unique_field" and event.type in ("entity.create", "entity.update"):
            coll_id = constraint.get("collection")
            field_name = constraint.get("field")
            if coll_id and field_name:
                coll = _get_collection(snap, coll_id)
                if coll:
                    values_seen: list[Any] = []
                    for ent in coll["entities"].values():
                        if ent.get("_removed"):
                            continue
                        val = ent.get(field_name)
                        if val is not None and val in values_seen:
                            violated = True
                            break
                        if val is not None:
                            values_seen.append(val)

        elif rule == "exclude_pair" and event.type == "relationship.set":
            # Two entities must NOT share the same target
            # Only check if this event involves one of the constrained entities
            rel_type = constraint.get("relationship_type")
            entities = constraint.get("entities", [])
            event_from = event.payload.get("from")
            event_rel_type = event.payload.get("type")

            if rel_type and len(entities) == 2 and event_rel_type == rel_type:
                # Only check if this event's "from" entity is one of the constrained entities
                if event_from in entities:
                    # Find what targets each entity has for this relationship type
                    targets: dict[str, str | None] = {e: None for e in entities}
                    for rel in snap["relationships"]:
                        if rel.get("_excluded"):
                            continue
                        if rel.get("type") == rel_type and rel.get("from") in targets:
                            targets[rel["from"]] = rel.get("to")
                    # If both entities have the same target, violated
                    target_vals = [t for t in targets.values() if t is not None]
                    if len(target_vals) == 2 and target_vals[0] == target_vals[1]:
                        violated = True

        elif rule == "require_same" and event.type == "relationship.set":
            # Two entities MUST share the same target
            # Only check if this event involves one of the constrained entities
            rel_type = constraint.get("relationship_type")
            entities = constraint.get("entities", [])
            event_from = event.payload.get("from")
            event_rel_type = event.payload.get("type")

            if rel_type and len(entities) == 2 and event_rel_type == rel_type:
                if event_from in entities:
                    targets: dict[str, str | None] = {e: None for e in entities}
                    for rel in snap["relationships"]:
                        if rel.get("_excluded"):
                            continue
                        if rel.get("type") == rel_type and rel.get("from") in targets:
                            targets[rel["from"]] = rel.get("to")
                    # Both must have a target and they must match
                    t1, t2 = targets.get(entities[0]), targets.get(entities[1])
                    if t1 is not None and t2 is not None and t1 != t2:
                        violated = True

        elif rule == "max_per_target" and event.type == "relationship.set":
            # No target can have more than N sources
            rel_type = constraint.get("relationship_type")
            max_val = constraint.get("value")
            if rel_type and max_val is not None:
                # Count sources per target
                target_counts: dict[str, int] = {}
                for rel in snap["relationships"]:
                    if rel.get("_excluded"):
                        continue
                    if rel.get("type") == rel_type:
                        to_ref = rel.get("to")
                        target_counts[to_ref] = target_counts.get(to_ref, 0) + 1
                # Check if any target exceeds max
                for count in target_counts.values():
                    if count > max_val:
                        violated = True
                        break

        elif rule == "min_per_target" and event.type == "relationship.set":
            # Check if any previously-populated target now drops below min
            rel_type = constraint.get("relationship_type")
            min_val = constraint.get("value")
            if rel_type and min_val is not None:
                # Count current sources for each target that this entity might have left
                target_counts: dict[str, int] = {}
                for rel in snap["relationships"]:
                    if rel.get("_excluded"):
                        continue
                    if rel.get("type") == rel_type:
                        to_ref = rel.get("to")
                        target_counts[to_ref] = target_counts.get(to_ref, 0) + 1
                # Check targets that had relationships before - if any dropped below min
                for _to_ref, count in target_counts.items():
                    if count < min_val:
                        violated = True
                        break

        elif rule == "required_fields" and event.type == "entity.update":
            # Check if any monitored field is being set to null
            coll_id = constraint.get("collection")
            req_fields = constraint.get("fields") or []
            if event.payload.get("ref"):
                ref_coll, _ = parse_ref(event.payload["ref"])
                if ref_coll == coll_id:
                    update_fields = event.payload.get("fields", {})
                    for field in req_fields:
                        if field in update_fields and update_fields[field] is None:
                            violated = True
                            break

        elif rule == "required_fields" and event.type == "field.remove":
            # Warn if removing a field referenced by required_fields constraint
            coll_id = constraint.get("collection")
            req_fields = constraint.get("fields") or []
            if event.payload.get("collection") == coll_id:
                if event.payload.get("name") in req_fields:
                    violated = True
                    msg = f"Removing field '{event.payload.get('name')}' referenced by required_fields constraint"

        if violated:
            if strict:
                return False  # Caller should reject
            warnings.append(Warning(code="CONSTRAINT_VIOLATED", message=msg))

    return True


# ---------------------------------------------------------------------------
# Primitive handlers
# ---------------------------------------------------------------------------


def _handle_entity_create(snap: dict, event: Event) -> ReduceResult:
    p = event.payload
    coll_id = p["collection"]
    entity_id = p.get("id")
    fields = p.get("fields", {})
    warnings: list[Warning] = []

    # 1. Collection must exist
    coll = _get_collection(snap, coll_id)
    if coll is None:
        return _reject(snap, "COLLECTION_NOT_FOUND", coll_id)

    # 2. Auto-generate ID if missing
    if entity_id is None:
        counter = len(coll["entities"]) + 1
        entity_id = f"{coll_id}_{counter}"
        while entity_id in coll["entities"]:
            counter += 1
            entity_id = f"{coll_id}_{counter}"

    # 3. Check ID doesn't already exist (unless re-creating removed entity)
    existing = coll["entities"].get(entity_id)
    if existing is not None and not existing.get("_removed"):
        return _reject(snap, "ENTITY_ALREADY_EXISTS", f"{coll_id}/{entity_id}")

    # 4. Validate fields against schema
    schema = coll.get("schema", {})
    entity_fields: dict[str, Any] = {}

    for field_name, field_type in schema.items():
        if field_name in fields:
            valid, err = _validate_field_value(fields[field_name], field_type)
            if not valid:
                return _reject(snap, "TYPE_MISMATCH", f"{field_name}: {err}")
            entity_fields[field_name] = fields[field_name]
        elif is_nullable_type(field_type):
            entity_fields[field_name] = None
        else:
            return _reject(snap, "REQUIRED_FIELD_MISSING", f"'{field_name}' is required")

    # Warn about extra fields not in schema
    for key in fields:
        if key not in schema:
            warnings.append(
                Warning(
                    code="UNKNOWN_FIELD_IGNORED",
                    message=f"Field '{key}' not in schema, ignored",
                )
            )

    # 5. Store entity
    entity_fields["_removed"] = False
    entity_fields["_created_seq"] = event.sequence
    coll["entities"][entity_id] = entity_fields

    # 6. Check constraints
    if not _check_constraints(snap, event, warnings):
        # Undo the creation for strict constraint violation
        del coll["entities"][entity_id]
        return _reject(snap, "STRICT_CONSTRAINT_VIOLATED", "entity.create violated strict constraint")

    return _ok(snap, warnings)


def _handle_entity_update(snap: dict, event: Event) -> ReduceResult:
    p = event.payload
    fields = p.get("fields", {})
    warnings: list[Warning] = []

    if "ref" in p:
        # Single entity update
        coll_id, entity_id = parse_ref(p["ref"])
        coll, entity = _get_entity(snap, coll_id, entity_id)
        if coll is None:
            return _reject(snap, "COLLECTION_NOT_FOUND", coll_id)
        if entity is None:
            return _reject(snap, "ENTITY_NOT_FOUND", p["ref"])

        schema = coll.get("schema", {})
        for key, value in fields.items():
            if key in schema:
                valid, err = _validate_field_value(value, schema[key])
                if not valid:
                    return _reject(snap, "TYPE_MISMATCH", f"{key}: {err}")
            entity[key] = value

        entity["_updated_seq"] = event.sequence

    elif "filter" in p:
        # Bulk update via filter
        filt = p["filter"]
        coll_id = filt["collection"]
        where = filt.get("where", {})

        coll = _get_collection(snap, coll_id)
        if coll is None:
            return _reject(snap, "COLLECTION_NOT_FOUND", coll_id)

        schema = coll.get("schema", {})
        count = 0

        for entity in coll["entities"].values():
            if entity.get("_removed"):
                continue
            # Check filter match
            if all(entity.get(k) == v for k, v in where.items()):
                for key, value in fields.items():
                    if key in schema:
                        valid, err = _validate_field_value(value, schema[key])
                        if not valid:
                            return _reject(snap, "TYPE_MISMATCH", f"{key}: {err}")
                    entity[key] = value
                entity["_updated_seq"] = event.sequence
                count += 1

        warnings.append(
            Warning(
                code="ENTITIES_AFFECTED",
                message=f"{count} entities updated",
            )
        )

    # Check constraints
    if not _check_constraints(snap, event, warnings):
        return _reject(snap, "STRICT_CONSTRAINT_VIOLATED", "entity.update violated strict constraint")

    return _ok(snap, warnings)


def _handle_entity_remove(snap: dict, event: Event) -> ReduceResult:
    p = event.payload
    coll_id, entity_id = parse_ref(p["ref"])
    warnings: list[Warning] = []

    coll = snap["collections"].get(coll_id)
    if coll is None or coll.get("_removed"):
        return _reject(snap, "COLLECTION_NOT_FOUND", coll_id)

    entity = coll["entities"].get(entity_id)
    if entity is None:
        return _reject(snap, "ENTITY_NOT_FOUND", p["ref"])

    if entity.get("_removed"):
        warnings.append(Warning(code="ALREADY_REMOVED", message=f"{p['ref']} already removed"))
        return _ok(snap, warnings)

    entity["_removed"] = True
    entity["_removed_seq"] = event.sequence

    # Exclude relationships involving this entity
    ref = p["ref"]
    for rel in snap["relationships"]:
        if rel.get("from") == ref or rel.get("to") == ref:
            rel["_excluded"] = True

    return _ok(snap, warnings)


def _handle_collection_create(snap: dict, event: Event) -> ReduceResult:
    p = event.payload
    coll_id = p["id"]

    existing = snap["collections"].get(coll_id)
    if existing is not None and not existing.get("_removed"):
        return _reject(snap, "COLLECTION_ALREADY_EXISTS", coll_id)

    # Validate schema types
    schema = p.get("schema", {})
    for field_name, field_type in schema.items():
        if not is_valid_field_type(field_type):
            return _reject(snap, "TYPE_MISMATCH", f"Invalid schema type for '{field_name}': {field_type}")

    snap["collections"][coll_id] = {
        "id": coll_id,
        "name": p.get("name", coll_id.replace("_", " ").title()),
        "schema": schema,
        "settings": p.get("settings", {}),
        "entities": {},
        "_removed": False,
        "_created_seq": event.sequence,
    }

    return _ok(snap)


def _handle_collection_update(snap: dict, event: Event) -> ReduceResult:
    p = event.payload
    coll_id = p["id"]

    coll = _get_collection(snap, coll_id)
    if coll is None:
        return _reject(snap, "COLLECTION_NOT_FOUND", coll_id)

    if "name" in p:
        coll["name"] = p["name"]
    if "settings" in p:
        settings = coll.get("settings", {})
        for k, v in p["settings"].items():
            if v is None:
                settings.pop(k, None)
            else:
                settings[k] = v
        coll["settings"] = settings

    return _ok(snap)


def _handle_collection_remove(snap: dict, event: Event) -> ReduceResult:
    p = event.payload
    coll_id = p["id"]
    warnings: list[Warning] = []

    coll = snap["collections"].get(coll_id)
    if coll is None:
        return _reject(snap, "COLLECTION_NOT_FOUND", coll_id)
    if coll.get("_removed"):
        warnings.append(Warning(code="ALREADY_REMOVED", message=f"Collection '{coll_id}' already removed"))
        return _ok(snap, warnings)

    # Soft-delete collection and all entities
    coll["_removed"] = True
    for entity in coll["entities"].values():
        entity["_removed"] = True

    # Exclude relationships involving entities in this collection
    for rel in snap["relationships"]:
        if rel.get("from", "").startswith(f"{coll_id}/") or rel.get("to", "").startswith(f"{coll_id}/"):
            rel["_excluded"] = True

    # Remove views sourced from this collection
    for view in snap["views"].values():
        if view.get("source") == coll_id:
            view["_removed"] = True

    # Remove collection_view blocks referencing this collection
    blocks_to_remove = []
    for block_id, block in snap["blocks"].items():
        if block.get("type") == "collection_view":
            props = block.get("props", {})
            if props.get("source") == coll_id:
                blocks_to_remove.append(block_id)

    # Remove blocks from their parents and delete them
    for block_id in blocks_to_remove:
        block = snap["blocks"].get(block_id)
        if block:
            parent_id = block.get("parent", "block_root")
            parent = snap["blocks"].get(parent_id)
            if parent and block_id in parent.get("children", []):
                parent["children"].remove(block_id)
            del snap["blocks"][block_id]

    return _ok(snap, warnings)


def _handle_grid_create(snap: dict, event: Event) -> ReduceResult:
    """
    Create a grid of entities in one operation.
    Payload: { collection, rows, cols, defaults? }
    Creates rows × cols entities with row/col fields.
    """
    p = event.payload
    coll_id = p["collection"]
    rows = p["rows"]
    cols = p["cols"]
    defaults = p.get("defaults", {})

    coll = _get_collection(snap, coll_id)
    if coll is None:
        return _reject(snap, "COLLECTION_NOT_FOUND", coll_id)

    schema = coll.get("schema", {})

    # Verify schema has row and col fields
    if "row" not in schema or "col" not in schema:
        return _reject(snap, "SCHEMA_MISMATCH", "Grid requires 'row' and 'col' fields in schema")

    # Create all grid entities
    for row in range(rows):
        for col in range(cols):
            entity_id = f"cell_{row}_{col}"

            # Build entity fields from schema
            entity_fields: dict[str, Any] = {"row": row, "col": col}
            for field_name, field_type in schema.items():
                if field_name in ("row", "col"):
                    continue
                if field_name in defaults:
                    entity_fields[field_name] = defaults[field_name]
                elif is_nullable_type(field_type):
                    entity_fields[field_name] = None
                # Required fields without defaults will fail - but grid.create assumes nullable fields

            entity_fields["_removed"] = False
            entity_fields["_created_seq"] = event.sequence
            coll["entities"][entity_id] = entity_fields

    return _ok(snap)


def _handle_field_add(snap: dict, event: Event) -> ReduceResult:
    p = event.payload
    coll_id = p["collection"]
    field_name = p["name"]
    field_type = p["type"]

    coll = _get_collection(snap, coll_id)
    if coll is None:
        return _reject(snap, "COLLECTION_NOT_FOUND", coll_id)

    schema = coll.get("schema", {})
    if field_name in schema:
        return _reject(snap, "FIELD_ALREADY_EXISTS", f"'{field_name}' in '{coll_id}'")

    # Required field without default → reject (unless collection is empty)
    has_entities = any(not e.get("_removed") for e in coll["entities"].values())
    if not is_nullable_type(field_type) and "default" not in p and has_entities:
        return _reject(snap, "REQUIRED_FIELD_NO_DEFAULT", f"Can't add required field '{field_name}' without default")

    schema[field_name] = field_type

    # Backfill existing entities (including removed, for undo/replay support)
    default = p.get("default", None)
    for entity in coll["entities"].values():
        entity[field_name] = default

    return _ok(snap)


def _handle_field_update(snap: dict, event: Event) -> ReduceResult:
    p = event.payload
    coll_id = p["collection"]
    field_name = p["name"]

    coll = _get_collection(snap, coll_id)
    if coll is None:
        return _reject(snap, "COLLECTION_NOT_FOUND", coll_id)

    schema = coll.get("schema", {})
    if field_name not in schema:
        return _reject(snap, "FIELD_NOT_FOUND", f"'{field_name}' in '{coll_id}'")

    warnings: list[Warning] = []
    old_type = schema[field_name]

    # Handle type change
    if "type" in p:
        new_type = p["type"]

        # Type compatibility checking (before validity check for better error messages)
        old_base = base_type(old_type)

        # Handle bare "list" string (should be {"list": "type"})
        if new_type == "list" or (isinstance(new_type, dict) and "list" in new_type):
            if old_base in ("string", "bool", "date", "datetime", "int", "float", "enum"):
                return _reject(snap, "INCOMPATIBLE_TYPE_CHANGE", f"Cannot convert {old_base} to list")

        if not is_valid_field_type(new_type):
            return _reject(snap, "TYPE_MISMATCH", f"Invalid type: {new_type}")

        new_base = base_type(new_type)

        # Get existing non-null values
        existing_values = []
        for entity in coll["entities"].values():
            if entity.get("_removed"):
                continue
            val = entity.get(field_name)
            if val is not None:
                existing_values.append(val)

        # Check compatibility based on type transition
        if old_base == "enum" and new_base not in ("enum", "string"):
            # enum → anything other than string is rejected
            return _reject(snap, "INCOMPATIBLE_TYPE_CHANGE", f"Cannot convert enum to {new_base}")

        if old_base == "list":
            # list → anything else is rejected
            return _reject(snap, "INCOMPATIBLE_TYPE_CHANGE", f"Cannot convert list to {new_base}")

        if old_base == "float" and new_base == "int" and existing_values:
            # float → int: check for lossy conversion
            for val in existing_values:
                if isinstance(val, float) and val != int(val):
                    warnings.append(
                        Warning(
                            code="LOSSY_TYPE_CONVERSION",
                            message="Converting float to int will truncate decimal values",
                        )
                    )
                    break

        if old_base == "string" and new_base == "int" and existing_values:
            # string → int: check all values are numeric
            for val in existing_values:
                if isinstance(val, str):
                    try:
                        int(val)
                    except ValueError:
                        return _reject(snap, "INCOMPATIBLE_TYPE_CHANGE", f"Cannot convert '{val}' to int")

        if old_base == "string" and new_base == "enum" and existing_values:
            # string → enum: check all values are in enum
            if isinstance(new_type, dict) and "enum" in new_type:
                allowed = set(new_type["enum"])
                for val in existing_values:
                    if val not in allowed:
                        return _reject(
                            snap, "INCOMPATIBLE_TYPE_CHANGE", f"Value '{val}' not in enum {new_type['enum']}"
                        )

        if old_base in ("string", "bool", "date", "datetime") and new_base == "list":
            return _reject(snap, "INCOMPATIBLE_TYPE_CHANGE", f"Cannot convert {old_base} to list")

        if old_base == "date" and new_base == "int":
            return _reject(snap, "INCOMPATIBLE_TYPE_CHANGE", "Cannot convert date to int")

        if old_base == "bool" and new_base == "float":
            return _reject(snap, "INCOMPATIBLE_TYPE_CHANGE", "Cannot convert bool to float")

        schema[field_name] = new_type

    # Handle rename
    if "rename" in p:
        new_name = p["rename"]
        if new_name in schema:
            return _reject(snap, "FIELD_ALREADY_EXISTS", f"'{new_name}' in '{coll_id}'")
        schema[new_name] = schema.pop(field_name)
        for entity in coll["entities"].values():
            if field_name in entity:
                entity[new_name] = entity.pop(field_name)

    return _ok(snap, warnings)


def _handle_field_remove(snap: dict, event: Event) -> ReduceResult:
    p = event.payload
    coll_id = p["collection"]
    field_name = p["name"]
    warnings: list[Warning] = []

    coll = _get_collection(snap, coll_id)
    if coll is None:
        return _reject(snap, "COLLECTION_NOT_FOUND", coll_id)

    schema = coll.get("schema", {})
    if field_name not in schema:
        return _reject(snap, "FIELD_NOT_FOUND", f"'{field_name}' in '{coll_id}'")

    del schema[field_name]

    # Remove from all entities
    for entity in coll["entities"].values():
        entity.pop(field_name, None)

    # Warn about views referencing this field
    for view in snap["views"].values():
        if view.get("source") != coll_id:
            continue
        config = view.get("config", {})
        for config_key in ("show_fields", "hide_fields"):
            if field_name in config.get(config_key, []):
                config[config_key] = [f for f in config[config_key] if f != field_name]
                warnings.append(
                    Warning(
                        code="VIEW_FIELD_MISSING",
                        message=f"View '{view['id']}' referenced removed field '{field_name}'",
                    )
                )
        for config_key in ("sort_by", "group_by"):
            if config.get(config_key) == field_name:
                config.pop(config_key, None)
                warnings.append(
                    Warning(
                        code="VIEW_FIELD_MISSING",
                        message=f"View '{view['id']}' referenced removed field '{field_name}'",
                    )
                )

    # Check constraints (e.g., required_fields referencing this field)
    _check_constraints(snap, event, warnings)

    return _ok(snap, warnings)


def _handle_relationship_set(snap: dict, event: Event) -> ReduceResult:
    p = event.payload
    from_ref = p["from"]
    to_ref = p["to"]
    rel_type = p["type"]
    cardinality = p.get("cardinality", "many_to_one")
    data = p.get("data", {})

    # Resolve entities
    from_coll, from_ent = parse_ref(from_ref)
    to_coll, to_ent = parse_ref(to_ref)
    _, from_entity = _get_entity(snap, from_coll, from_ent)
    _, to_entity = _get_entity(snap, to_coll, to_ent)

    if from_entity is None:
        return _reject(snap, "ENTITY_NOT_FOUND", from_ref)
    if to_entity is None:
        return _reject(snap, "ENTITY_NOT_FOUND", to_ref)

    # Register or lookup relationship type
    if rel_type not in snap["relationship_types"]:
        snap["relationship_types"][rel_type] = {"cardinality": cardinality}
    stored_cardinality = snap["relationship_types"][rel_type]["cardinality"]

    # Enforce cardinality
    # many_to_one: many sources can point to one target, each source has only ONE target
    # one_to_one:  each source has one target AND each target has one source
    # many_to_many: no restrictions
    if stored_cardinality == "many_to_one":
        # Remove existing relationships from this source of this type (each source has one target)
        snap["relationships"] = [
            r
            for r in snap["relationships"]
            if not (r["from"] == from_ref and r["type"] == rel_type and not r.get("_excluded"))
        ]
    elif stored_cardinality == "one_to_one":
        # Remove both sides
        snap["relationships"] = [
            r
            for r in snap["relationships"]
            if not (
                (r["from"] == from_ref and r["type"] == rel_type and not r.get("_excluded"))
                or (r["to"] == to_ref and r["type"] == rel_type and not r.get("_excluded"))
            )
        ]
    # many_to_many: no auto-removal

    # Append new relationship
    snap["relationships"].append(
        {
            "from": from_ref,
            "to": to_ref,
            "type": rel_type,
            "data": data,
            "_seq": event.sequence,
        }
    )

    warnings: list[Warning] = []
    if not _check_constraints(snap, event, warnings):
        return _reject(snap, "STRICT_CONSTRAINT_VIOLATED", "relationship.set violated strict constraint")
    return _ok(snap, warnings)


def _handle_relationship_constrain(snap: dict, event: Event) -> ReduceResult:
    p = event.payload
    warnings: list[Warning] = []

    constraint = {
        "id": p["id"],
        "rule": p["rule"],
        "entities": p.get("entities"),
        "relationship_type": p.get("relationship_type"),
        "value": p.get("value"),
        "message": p.get("message"),
        "strict": p.get("strict", False),
    }

    # Replace if same ID exists, otherwise append
    existing_idx = None
    for i, c in enumerate(snap["constraints"]):
        if c.get("id") == p["id"]:
            existing_idx = i
            break
    if existing_idx is not None:
        snap["constraints"][existing_idx] = constraint
    else:
        snap["constraints"].append(constraint)

    return _ok(snap, warnings)


def _handle_block_set(snap: dict, event: Event) -> ReduceResult:
    p = event.payload
    block_id = p["id"]
    blocks = snap["blocks"]
    parent_id = p.get("parent", "block_root")
    position = p.get("position")

    existing = blocks.get(block_id)

    if existing is None:
        # CREATE mode
        if "type" not in p:
            return _reject(snap, "BLOCK_TYPE_MISSING", f"block.set (create) requires 'type' for '{block_id}'")
        if parent_id not in blocks:
            return _reject(snap, "BLOCK_NOT_FOUND", f"Parent '{parent_id}' not found")

        new_block = {
            "id": block_id,
            "type": p["type"],
            "parent": parent_id,
            "props": p.get("props", {}),
            "children": [],
        }
        blocks[block_id] = new_block

        # Insert into parent's children
        parent = blocks[parent_id]
        if position is not None and 0 <= position <= len(parent["children"]):
            parent["children"].insert(position, block_id)
        else:
            parent["children"].append(block_id)

    else:
        # UPDATE mode
        if "props" in p:
            existing_props = existing.get("props", {})
            existing_props.update(p["props"])
            existing["props"] = existing_props

        if "type" in p:
            existing["type"] = p["type"]

        # Handle reparenting
        if "parent" in p and p["parent"] != existing.get("parent"):
            old_parent_id = existing.get("parent", "block_root")
            new_parent_id = p["parent"]
            if new_parent_id not in blocks:
                return _reject(snap, "BLOCK_NOT_FOUND", f"Parent '{new_parent_id}' not found")

            # Remove from old parent
            old_parent = blocks.get(old_parent_id)
            if old_parent and block_id in old_parent["children"]:
                old_parent["children"].remove(block_id)

            # Insert into new parent
            new_parent = blocks[new_parent_id]
            if position is not None and 0 <= position <= len(new_parent["children"]):
                new_parent["children"].insert(position, block_id)
            else:
                new_parent["children"].append(block_id)

            existing["parent"] = new_parent_id

    return _ok(snap)


def _handle_block_remove(snap: dict, event: Event) -> ReduceResult:
    p = event.payload
    block_id = p["id"]
    blocks = snap["blocks"]

    if block_id == "block_root":
        return _reject(snap, "CANT_REMOVE_ROOT", "Cannot remove block_root")

    if block_id not in blocks:
        return _reject(snap, "BLOCK_NOT_FOUND", block_id)

    # Collect block and all descendants
    to_remove: list[str] = []

    def collect(bid: str) -> None:
        to_remove.append(bid)
        block = blocks.get(bid)
        if block:
            for child in block.get("children", []):
                collect(child)

    collect(block_id)

    # Remove from parent
    block = blocks[block_id]
    parent_id = block.get("parent", "block_root")
    parent = blocks.get(parent_id)
    if parent and block_id in parent["children"]:
        parent["children"].remove(block_id)

    # Delete all collected blocks
    for bid in to_remove:
        blocks.pop(bid, None)

    return _ok(snap)


def _handle_block_reorder(snap: dict, event: Event) -> ReduceResult:
    p = event.payload
    parent_id = p["parent"]
    new_order = p["children"]
    blocks = snap["blocks"]
    warnings: list[Warning] = []

    if parent_id not in blocks:
        return _reject(snap, "BLOCK_NOT_FOUND", f"Parent '{parent_id}' not found")

    parent = blocks[parent_id]
    current_children = set(parent["children"])

    # Validate: warn about IDs not in current children
    valid_order: list[str] = []
    for cid in new_order:
        if cid in current_children:
            valid_order.append(cid)
        else:
            warnings.append(Warning(code="UNKNOWN_FIELD_IGNORED", message=f"'{cid}' is not a child of '{parent_id}'"))

    # Append any current children not in the provided order
    for cid in parent["children"]:
        if cid not in valid_order:
            valid_order.append(cid)

    parent["children"] = valid_order
    return _ok(snap, warnings)


def _handle_view_create(snap: dict, event: Event) -> ReduceResult:
    p = event.payload
    view_id = p["id"]
    warnings: list[Warning] = []

    if view_id in snap["views"]:
        return _reject(snap, "VIEW_ALREADY_EXISTS", view_id)

    source = p["source"]
    coll = _get_collection(snap, source)
    if coll is None:
        return _reject(snap, "COLLECTION_NOT_FOUND", source)

    # Warn about config referencing nonexistent schema fields
    config = p.get("config", {})
    schema = coll.get("schema", {})
    for field_ref_key in ("show_fields", "hide_fields"):
        for f in config.get(field_ref_key, []):
            if f not in schema:
                warnings.append(
                    Warning(code="VIEW_FIELD_MISSING", message=f"View '{view_id}' references field '{f}' not in schema")
                )

    snap["views"][view_id] = {
        "id": view_id,
        "type": p["type"],
        "source": source,
        "config": config,
    }

    return _ok(snap, warnings)


def _handle_view_update(snap: dict, event: Event) -> ReduceResult:
    p = event.payload
    view_id = p["id"]

    view = snap["views"].get(view_id)
    if view is None:
        return _reject(snap, "VIEW_NOT_FOUND", view_id)

    if "type" in p:
        view["type"] = p["type"]
    if "config" in p:
        existing_config = view.get("config", {})
        existing_config.update(p["config"])
        view["config"] = existing_config

    return _ok(snap)


def _handle_view_remove(snap: dict, event: Event) -> ReduceResult:
    p = event.payload
    view_id = p["id"]
    warnings: list[Warning] = []

    if view_id not in snap["views"]:
        return _reject(snap, "VIEW_NOT_FOUND", view_id)

    del snap["views"][view_id]

    # Null out block references to this view
    for block in snap["blocks"].values():
        if block.get("type") == "collection_view":
            props = block.get("props", {})
            if props.get("view") == view_id:
                props["view"] = None
                warnings.append(
                    Warning(
                        code="BLOCK_VIEW_MISSING", message=f"Block '{block['id']}' referenced removed view '{view_id}'"
                    )
                )

    return _ok(snap, warnings)


def _handle_style_set(snap: dict, event: Event) -> ReduceResult:
    p = event.payload
    for key, value in p.items():
        if value is None:
            snap["styles"].pop(key, None)
        else:
            snap["styles"][key] = value
    return _ok(snap)


def _handle_style_set_entity(snap: dict, event: Event) -> ReduceResult:
    p = event.payload
    ref = p["ref"]
    styles = p["styles"]

    coll_id, entity_id = parse_ref(ref)
    _, entity = _get_entity(snap, coll_id, entity_id)
    if entity is None:
        return _reject(snap, "ENTITY_NOT_FOUND", ref)

    existing_styles = entity.get("_styles", {})
    existing_styles.update(styles)
    entity["_styles"] = existing_styles

    return _ok(snap)


def _handle_meta_update(snap: dict, event: Event) -> ReduceResult:
    p = event.payload
    for key, value in p.items():
        snap["meta"][key] = value
    return _ok(snap)


def _handle_meta_annotate(snap: dict, event: Event) -> ReduceResult:
    p = event.payload
    snap["annotations"].append(
        {
            "note": p["note"],
            "pinned": p.get("pinned", False),
            "seq": event.sequence,
            "timestamp": event.timestamp,
        }
    )
    return _ok(snap)


def _handle_meta_constrain(snap: dict, event: Event) -> ReduceResult:
    p = event.payload
    warnings: list[Warning] = []

    constraint = {
        "id": p["id"],
        "rule": p["rule"],
        "collection": p.get("collection"),
        "field": p.get("field"),
        "fields": p.get("fields"),  # for required_fields constraint
        "value": p.get("value"),
        "message": p.get("message"),
        "strict": p.get("strict", False),
    }

    # Replace if same ID exists, otherwise append
    existing_idx = None
    for i, c in enumerate(snap["constraints"]):
        if c.get("id") == p["id"]:
            existing_idx = i
            break
    if existing_idx is not None:
        snap["constraints"][existing_idx] = constraint
    else:
        snap["constraints"].append(constraint)

    # Immediate validation
    if p["rule"] == "collection_max_entities":
        coll_id = p.get("collection")
        max_val = p.get("value")
        if coll_id and max_val is not None:
            coll = _get_collection(snap, coll_id)
            if coll:
                count = sum(1 for e in coll["entities"].values() if not e.get("_removed"))
                if count > max_val:
                    msg = p.get("message", f"Collection already has {count} entities (max {max_val})")
                    warnings.append(Warning(code="CONSTRAINT_VIOLATED", message=msg))

    if p["rule"] == "unique_field":
        coll_id = p.get("collection")
        field_name = p.get("field")
        if coll_id and field_name:
            coll = _get_collection(snap, coll_id)
            if coll:
                seen: set[Any] = set()
                for ent in coll["entities"].values():
                    if ent.get("_removed"):
                        continue
                    val = ent.get(field_name)
                    if val is not None and val in seen:
                        warnings.append(
                            Warning(
                                code="CONSTRAINT_VIOLATED",
                                message=p.get("message", f"Duplicate value for '{field_name}'"),
                            )
                        )
                        break
                    if val is not None:
                        seen.add(val)

    return _ok(snap, warnings)


# ---------------------------------------------------------------------------
# Handler dispatch table
# ---------------------------------------------------------------------------

_HANDLERS: dict[str, Any] = {
    "entity.create": _handle_entity_create,
    "entity.update": _handle_entity_update,
    "entity.remove": _handle_entity_remove,
    "collection.create": _handle_collection_create,
    "collection.update": _handle_collection_update,
    "collection.remove": _handle_collection_remove,
    "grid.create": _handle_grid_create,
    "field.add": _handle_field_add,
    "field.update": _handle_field_update,
    "field.remove": _handle_field_remove,
    "relationship.set": _handle_relationship_set,
    "relationship.constrain": _handle_relationship_constrain,
    "block.set": _handle_block_set,
    "block.remove": _handle_block_remove,
    "block.reorder": _handle_block_reorder,
    "view.create": _handle_view_create,
    "view.update": _handle_view_update,
    "view.remove": _handle_view_remove,
    "style.set": _handle_style_set,
    "style.set_entity": _handle_style_set_entity,
    "meta.update": _handle_meta_update,
    "meta.annotate": _handle_meta_annotate,
    "meta.constrain": _handle_meta_constrain,
}


# ---------------------------------------------------------------------------
# Query Helpers
# ---------------------------------------------------------------------------


def apply_sort(entities: list[dict[str, Any]], cfg: dict[str, Any]) -> list[dict[str, Any]]:
    """Sort entities by a field specified in view config."""
    sort_by = cfg.get("sort_by")
    if not sort_by:
        return entities
    reverse = cfg.get("sort_order") == "desc"

    def sort_key(e: dict[str, Any]) -> tuple[int, Any]:
        val = e.get(sort_by)
        # Nulls sort last
        if val is None:
            return (1, "")
        return (0, val)

    return sorted(entities, key=sort_key, reverse=reverse)


def apply_filter(entities: list[dict[str, Any]], cfg: dict[str, Any]) -> list[dict[str, Any]]:
    """Filter entities by field values specified in view config."""
    filter_cfg = cfg.get("filter")
    if not filter_cfg:
        return entities
    return [e for e in entities if all(e.get(k) == v for k, v in filter_cfg.items())]


def resolve_view_entities(snapshot: dict[str, Any], view_id: str) -> list[dict[str, Any]]:
    """Get entities for a view with sorting and filtering applied."""
    views = snapshot.get("views", {})
    view = views.get(view_id)
    if not view:
        return []

    collections = snapshot.get("collections", {})
    coll = collections.get(view.get("source", ""))
    if not coll or coll.get("_removed"):
        return []

    entities = [e for e in coll.get("entities", {}).values() if not e.get("_removed")]
    cfg = view.get("config", {})
    entities = apply_sort(entities, cfg)
    entities = apply_filter(entities, cfg)
    return entities


def resolve_view_fields(snapshot: dict[str, Any], view_id: str) -> list[str]:
    """Get visible fields for a view."""
    views = snapshot.get("views", {})
    view = views.get(view_id)
    if not view:
        return []

    collections = snapshot.get("collections", {})
    coll = collections.get(view.get("source", ""))
    if not coll or coll.get("_removed"):
        return []

    cfg = view.get("config", {})
    if "show_fields" in cfg:
        return cfg["show_fields"]

    # Default: all non-internal fields
    return [f for f in coll.get("schema", {}).keys() if not f.startswith("_")]


# ---------------------------------------------------------------------------
# (renderer section removed - React handles rendering)
# ---------------------------------------------------------------------------


# ---------------------------------------------------------------------------
# assembly.py
# ---------------------------------------------------------------------------

"""

AIde Kernel — Assembly Layer

Sits between the pure functions (reducer) and the outside world
(R2 storage, the orchestrator). Coordinates the lifecycle of an aide's HTML file.

Operations: load, apply, save, create, publish, fork

This is where IO happens. The reducer is pure.

Reference: aide_assembly_spec.md

"""


import asyncio
import copy
import json
import re
import uuid
from typing import Any


# ---------------------------------------------------------------------------
# Exceptions
# ---------------------------------------------------------------------------


class AideNotFound(Exception):
    """Aide does not exist in storage."""

    pass


class ParseError(Exception):
    """HTML file exists but embedded JSON is malformed."""

    pass


class VersionNotSupported(Exception):
    """Snapshot version is from a future format."""

    pass


# ---------------------------------------------------------------------------
# Storage protocol
# ---------------------------------------------------------------------------


class AideStorage:
    """
    Abstract storage interface.
    Implement with R2 for production, or in-memory for tests.
    """

    async def get(self, aide_id: str) -> str | None:
        """Fetch HTML file for an aide. Returns None if not found."""
        raise NotImplementedError

    async def put(self, aide_id: str, html: str) -> None:
        """Write HTML file for an aide (workspace bucket)."""
        raise NotImplementedError

    async def put_published(self, slug: str, html: str) -> None:
        """Write HTML file to published bucket."""
        raise NotImplementedError

    async def delete(self, aide_id: str) -> None:
        """Delete an aide's files from both buckets."""
        raise NotImplementedError


class MemoryStorage(AideStorage):
    """In-memory storage for testing."""

    def __init__(self) -> None:
        self.workspace: dict[str, str] = {}
        self.published: dict[str, str] = {}

    async def get(self, aide_id: str) -> str | None:
        return self.workspace.get(aide_id)

    async def put(self, aide_id: str, html: str) -> None:
        self.workspace[aide_id] = html

    async def put_published(self, slug: str, html: str) -> None:
        self.published[slug] = html

    async def delete(self, aide_id: str) -> None:
        self.workspace.pop(aide_id, None)
        self.published.pop(aide_id, None)


# ---------------------------------------------------------------------------
# AideAssembly
# ---------------------------------------------------------------------------


# ---------------------------------------------------------------------------
# HTML Parsing
# ---------------------------------------------------------------------------


def parse_aide_html(html: str) -> ParsedAide:
    """
    Extract blueprint, snapshot, and events from an aide HTML file.
    Uses regex on script tags — simple, no external dependency.
    """
    errors: list[str] = []
    blueprint: Blueprint | None = None
    snapshot: dict[str, Any] | None = None
    events: list[Event] = []

    # Extract blueprint
    bp_match = re.search(
        r'<script[^>]*id="aide-blueprint"[^>]*>(.*?)</script>',
        html,
        re.DOTALL,
    )
    if bp_match:
        try:
            bp_data = json.loads(bp_match.group(1).strip())
            blueprint = Blueprint.from_dict(bp_data)
        except (json.JSONDecodeError, KeyError) as e:
            errors.append(f"Failed to parse blueprint: {e}")

    # Extract snapshot
    state_match = re.search(
        r'<script[^>]*id="aide-state"[^>]*>(.*?)</script>',
        html,
        re.DOTALL,
    )
    if state_match:
        try:
            snapshot = json.loads(state_match.group(1).strip())
        except json.JSONDecodeError as e:
            errors.append(f"Failed to parse snapshot: {e}")

    # Extract events
    events_match = re.search(
        r'<script[^>]*id="aide-events"[^>]*>(.*?)</script>',
        html,
        re.DOTALL,
    )
    if events_match:
        try:
            events_data = json.loads(events_match.group(1).strip())
            events = [Event.from_dict(e) for e in events_data]
        except (json.JSONDecodeError, KeyError) as e:
            errors.append(f"Failed to parse events: {e}")

    return ParsedAide(
        blueprint=blueprint,
        snapshot=snapshot,
        events=events,
        parse_errors=errors,
    )


# ---------------------------------------------------------------------------
# Assembly class
# ---------------------------------------------------------------------------


class AideAssembly:
    """
    Manages the lifecycle of an aide's HTML file.
    Coordinates reducer + renderer + storage.
    """

    def __init__(self, storage: AideStorage):
        self._storage = storage
        self._locks: dict[str, asyncio.Lock] = {}

    def _get_lock(self, aide_id: str) -> asyncio.Lock:
        """Per-aide asyncio lock for single-instance serialization (v1)."""
        if aide_id not in self._locks:
            self._locks[aide_id] = asyncio.Lock()
        return self._locks[aide_id]

    # -- load --

    async def load(self, aide_id: str) -> AideFile:
        """
        Read an aide from R2.
        Fetches HTML, parses embedded JSON, returns AideFile.
        """
        html = await self._storage.get(aide_id)
        if html is None:
            raise AideNotFound(aide_id)

        parsed = parse_aide_html(html)
        if parsed.parse_errors:
            raise ParseError(f"Failed to parse aide {aide_id}: {parsed.parse_errors}")

        snapshot = parsed.snapshot or empty_state()
        events = parsed.events
        blueprint = parsed.blueprint or Blueprint(identity="")

        version = snapshot.get("version", 1)
        if version > 1:
            raise VersionNotSupported(f"Snapshot version {version} not supported")

        last_seq = max((e.sequence for e in events), default=0)

        return AideFile(
            aide_id=aide_id,
            snapshot=snapshot,
            events=events,
            blueprint=blueprint,
            html=html,
            last_sequence=last_seq,
            size_bytes=len(html.encode("utf-8")),
            loaded_from="r2",
        )

    # -- apply --

    async def apply(
        self,
        aide_file: AideFile,
        events: list[Event],
    ) -> ApplyResult:
        """
        Validate → reduce → re-render.
        Updates the AideFile in-memory. Call save() to persist.

        Partial application: rejected events are skipped, the rest still apply.
        """
        applied: list[Event] = []
        rejected: list[tuple[Event, str]] = []
        all_warnings: list[Warning] = []
        snapshot = aide_file.snapshot
        seq = aide_file.last_sequence

        for event in events:
            # Assign sequence if not already set
            if event.sequence == 0:
                seq += 1
                event.sequence = seq
                event.id = f"evt_{event.timestamp[:10].replace('-', '')}_{seq:03d}"

            # 1. Structural validation
            validation_errors = validate_primitive(event.type, event.payload)
            if validation_errors:
                rejected.append((event, "; ".join(validation_errors)))
                continue

            # 2. Reduce
            result: ReduceResult = reduce(snapshot, event)
            if not result.applied:
                rejected.append((event, result.error or "Unknown error"))
                continue

            # Applied
            snapshot = result.snapshot
            all_warnings.extend(result.warnings)
            applied.append(event)
            aide_file.events.append(event)

        aide_file.snapshot = snapshot
        aide_file.last_sequence = seq

        # Re-render if anything changed
        if applied:
            aide_file.html = render(
                snapshot,
                aide_file.blueprint,
                aide_file.events,
            )
            aide_file.size_bytes = len(aide_file.html.encode("utf-8"))

        return ApplyResult(
            aide_file=aide_file,
            applied=applied,
            rejected=rejected,
            warnings=all_warnings,
        )

    # -- save --

    async def save(self, aide_file: AideFile) -> None:
        """Write an aide back to R2 workspace bucket."""
        try:
            await self._storage.put(aide_file.aide_id, aide_file.html)
        except Exception:
            # Retry once per spec
            await self._storage.put(aide_file.aide_id, aide_file.html)

    # -- create --

    async def create(self, blueprint: Blueprint) -> AideFile:
        """
        Initialize a new aide with empty state.
        Does NOT save — caller persists after first L3 events.
        """
        aide_id = str(uuid.uuid4())
        snapshot = empty_state()

        # Title from first sentence of identity
        identity = blueprint.identity
        if identity:
            title = identity.split(".")[0].strip()[:100]
            snapshot["meta"]["title"] = title

        html = render(snapshot, blueprint, events=[])

        return AideFile(
            aide_id=aide_id,
            snapshot=snapshot,
            events=[],
            blueprint=blueprint,
            html=html,
            last_sequence=0,
            size_bytes=len(html.encode("utf-8")),
            loaded_from="new",
        )

    # -- publish --

    async def publish(
        self,
        aide_file: AideFile,
        *,
        slug: str | None = None,
        is_free_tier: bool = True,
    ) -> str:
        """
        Copy workspace file to published bucket.
        Returns the published URL.
        """
        if slug is None:
            slug = uuid.uuid4().hex[:8]

        footer = "Made with AIde" if is_free_tier else None
        options = RenderOptions(
            include_events=len(aide_file.events) <= 500,
            include_blueprint=True,
            footer=footer,
        )

        published_html = render(
            aide_file.snapshot,
            aide_file.blueprint,
            aide_file.events if options.include_events else None,
            options,
        )

        await self._storage.put_published(slug, published_html)
        return f"https://toaide.com/p/{slug}"

    # -- fork --

    async def fork(self, aide_id: str) -> AideFile:
        """
        Deep clone an aide's state and blueprint.
        Clears events, annotations, sequence metadata.
        Returns unsaved AideFile.
        """
        source = await self.load(aide_id)
        new_id = str(uuid.uuid4())
        snapshot = copy.deepcopy(source.snapshot)

        # Strip sequence metadata from entities
        for coll in snapshot.get("collections", {}).values():
            for entity in coll.get("entities", {}).values():
                entity.pop("_created_seq", None)
                entity.pop("_updated_seq", None)
                entity.pop("_removed_seq", None)

        # Update meta
        old_title = snapshot.get("meta", {}).get("title", "")
        if old_title:
            snapshot["meta"]["title"] = f"Copy of {old_title}"

        snapshot["annotations"] = []

        html = render(snapshot, source.blueprint, events=[])

        return AideFile(
            aide_id=new_id,
            snapshot=snapshot,
            events=[],
            blueprint=copy.deepcopy(source.blueprint),
            html=html,
            last_sequence=0,
            size_bytes=len(html.encode("utf-8")),
            loaded_from="new",
        )

    # -- integrity --

    async def integrity_check(self, aide_file: AideFile) -> tuple[bool, list[str]]:
        """Verify snapshot matches event replay."""

        replayed = replay(aide_file.events)
        stored = copy.deepcopy(aide_file.snapshot)
        stored.pop("version", None)
        replayed.pop("version", None)

        if json.dumps(stored, sort_keys=True) == json.dumps(replayed, sort_keys=True):
            return True, []
        return False, ["Snapshot does not match event replay"]

    async def repair(self, aide_file: AideFile) -> AideFile:
        """Rebuild snapshot from events, re-render HTML."""

        aide_file.snapshot = replay(aide_file.events)
        aide_file.snapshot["version"] = 1
        aide_file.html = render(aide_file.snapshot, aide_file.blueprint, aide_file.events)
        aide_file.size_bytes = len(aide_file.html.encode("utf-8"))
        return aide_file

    # -- compaction --

    async def compact(self, aide_file: AideFile, keep_recent: int = 50) -> AideFile:
        """
        Compact the event log. Keeps the most recent N events.
        Snapshot already reflects all events, so old events are redundant.
        """
        if len(aide_file.events) <= keep_recent:
            return aide_file

        aide_file.events = aide_file.events[-keep_recent:]
        aide_file.html = render(
            aide_file.snapshot,
            aide_file.blueprint,
            aide_file.events,
        )
        aide_file.size_bytes = len(aide_file.html.encode("utf-8"))
        return aide_file



# ---------------------------------------------------------------------------
# events.py
# ---------------------------------------------------------------------------

"""

AIde Kernel — Event Construction

Factory functions for creating well-formed events.
Used by the orchestrator to wrap primitives before feeding them to the reducer,
and by tests to build events concisely.

"""


from typing import Any



def make_event(
    seq: int,
    type: str,
    payload: dict[str, Any],
    *,
    actor: str = "user_test",
    source: str = "web",
    intent: str | None = None,
    message: str | None = None,
    message_id: str | None = None,
    timestamp: str | None = None,
    event_id: str | None = None,
) -> Event:
    """
    Build a complete Event from minimal inputs.

    seq is required — it determines both the event ID and sequence number.
    Everything else has sensible defaults for testing.
    """
    ts = timestamp or now_iso()
    eid = event_id or f"evt_{ts[:10].replace('-', '')}_{seq:03d}"

    return Event(
        id=eid,
        sequence=seq,
        timestamp=ts,
        actor=actor,
        source=source,
        type=type,
        payload=payload,
        intent=intent,
        message=message,
        message_id=message_id,
    )


def assign_metadata(
    events: list[dict[str, Any]],
    *,
    start_sequence: int,
    actor: str,
    source: str,
    message: str | None = None,
    message_id: str | None = None,
) -> list[Event]:
    """
    Assign metadata to a batch of raw primitives (type + payload dicts)
    coming from the AI compiler. Returns fully formed Events.

    Used by the assembly layer's apply() method.
    """
    ts = now_iso()
    result: list[Event] = []

    for i, primitive in enumerate(events):
        seq = start_sequence + i
        result.append(
            Event(
                id=f"evt_{ts[:10].replace('-', '')}_{seq:03d}",
                sequence=seq,
                timestamp=ts,
                actor=actor,
                source=source,
                type=primitive["type"],
                payload=primitive["payload"],
                intent=primitive.get("intent"),
                message=message if i == 0 else None,  # attach message to first event only
                message_id=message_id if i == 0 else None,
            )
        )

    return result



# ===========================================================================
# PUBLIC API
# ===========================================================================

__all__ = [
    # Core functions
    "empty_state",
    "reduce",
    "replay",
    "render",
    "render_block",
    # Assembly layer
    "AideAssembly",
    "AideStorage",
    "MemoryStorage",
    "parse_aide_html",
    # Types
    "Event",
    "Blueprint",
    "RenderOptions",
    "AideFile",
    "ApplyResult",
    "ReduceResult",
    "Warning",
    # Primitives
    "validate_primitive",
    "PRIMITIVE_TYPES",
]
